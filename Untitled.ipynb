{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1e3c2d-8dd3-4a0b-bc01-c707d7c09e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "collator = DataCollatorWithPadding(tokenizer, return_tensors = \"tf\")\n",
    "\n",
    "def tokenizing(inputs, prem_max_len, hypo_max_len, training) :\n",
    "    model_inputs = tokenizer(inputs['premise_chunk'], inputs[\"hypo_chunk\"])\n",
    "    \n",
    "    tokenized = model_inputs.input_ids\n",
    "\n",
    "    indices = []\n",
    "    \n",
    "    for t in tokenized :\n",
    "        t = np.array(t)\n",
    "        indices.append(np.where((t != 0) & (t != 2) & (t != 32000))[0])\n",
    "    \n",
    "    chunk_index = []\n",
    "    \n",
    "    for idx in indices :\n",
    "        temp = [1]\n",
    "        for i in range(len(idx) - 1) :\n",
    "            if idx[i] + 1 != idx[i + 1] :\n",
    "                temp.append(idx[i])\n",
    "                temp.append(idx[i + 1])\n",
    "        temp.append(idx[-1])\n",
    "        temp = temp + [-1 for i in range(idx_max_len * 2 - len(temp))]\n",
    "        temp  = np.array(temp).reshape(-1, 2).tolist()\n",
    "        chunk_index.append(temp)\n",
    "    \n",
    "    model_inputs[\"chunk_index\"] = chunk_index\n",
    "    \n",
    "    prem_gov_idx = []\n",
    "    prem_tag_info = []\n",
    "    \n",
    "    for i in range(len(inputs[\"premise_rel_gov_idx\"])) :\n",
    "        idx = inputs[\"premise_rel_gov_idx\"][i]\n",
    "        tag = inputs[\"premise_chunk_tag\"][i]\n",
    "        \n",
    "        prem_gov_idx.append(idx + [-1 for idx in range(prem_max_len - len(idx))])\n",
    "        temp = tag + [\"padding\" for tag in range(prem_max_len - len(tag))]\n",
    "        prem_tag_info.append(tag_labelr.transform(temp))\n",
    "\n",
    "    model_inputs[\"prem_gov_idx\"] = prem_gov_idx\n",
    "    model_inputs[\"prem_tag\"] = prem_tag_info\n",
    "    \n",
    "    \n",
    "    hypo_gov_idx = []\n",
    "    hypo_tag_info = []\n",
    "    \n",
    "    for i in range(len(inputs[\"hypo_rel_gov_idx\"])) :\n",
    "        idx = inputs[\"hypo_rel_gov_idx\"][i]\n",
    "        tag = inputs[\"hypo_chunk_tag\"][i]\n",
    "        \n",
    "        hypo_gov_idx.append(idx + [-1 for idx in range(hypo_max_len - len(idx))])\n",
    "        temp = tag + [\"padding\" for tag in range(hypo_max_len - len(tag))]\n",
    "        hypo_tag_info.append(tag_labelr.transform(temp))\n",
    "\n",
    "    model_inputs[\"hypo_gov_idx\"] = hypo_gov_idx\n",
    "    model_inputs[\"hypo_tag\"] = hypo_tag_info\n",
    "    \n",
    "    if training :\n",
    "        model_inputs[\"labels\"] = inputs[\"label\"]\n",
    "        \n",
    "\n",
    "    return model_inputs\n",
    "\n",
    "def get_dataset(inputs, collator, batch_size, idx_max_len, training) :\n",
    "    inputs = datasets.Dataset.from_pandas(inputs)\n",
    "    tokenized_inputs = inputs.map(tokenizing,\n",
    "                                  batched = True,\n",
    "                                  fn_kwargs = {\"training\" : training,\n",
    "                                               \"prem_max_len\" : premise_max_len,\n",
    "                                               \"hypo_max_len\" : hypo_max_len})\n",
    "\n",
    "    columns = [\"input_ids\", \"attention_mask\", \"token_type_ids\", \"chunk_index\", \"prem_gov_idx\", \"hypo_gov_idx\", \"prem_tag\", \"hypo_tag\"]\n",
    "    \n",
    "    if training :\n",
    "        inputs_data = tokenized_inputs.to_tf_dataset(\n",
    "            batch_size = batch_size,\n",
    "            columns = columns,\n",
    "            shuffle = True,\n",
    "            collate_fn = collator,\n",
    "            label_cols = \"labels\",\n",
    "            drop_remainder = False\n",
    "        )\n",
    "    else :\n",
    "        inputs_data = tokenized_inputs.to_tf_dataset(\n",
    "            batch_size = batch_size,\n",
    "            columns = columns,\n",
    "            shuffle = True,\n",
    "            collate_fn = collator,\n",
    "            drop_remainder = False\n",
    "        )\n",
    "        \n",
    "    return inputs_data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chunk_mean(premise_len, hypo_len, x, chunk_idx, sep_idx) :\n",
    "        premise = tf.TensorArray(tf.float32, size = 0, dynamic_size = True)\n",
    "        hypothesis = tf.TensorArray(tf.float32, size = 0, dynamic_size = True)\n",
    "        for batch in range(len(chunk_idx)) :\n",
    "            temp_premise = tf.TensorArray(tf.float32, size = 0, dynamic_size = True)\n",
    "            temp_hypothesis = tf.TensorArray(tf.float32, size = 0, dynamic_size = True)\n",
    "            for idx in chunk_idx[batch] :\n",
    "                if tf.reduce_sum(idx) > 0 :\n",
    "                    chunks = x[batch, idx[0] : idx[1], :] if idx[0] != idx[1] else tf.expand_dims(x[batch, idx[0], :], axis = 0)\n",
    "                    if sep_idx[batch, 1] > tf.cast(idx[1], tf.int64) :\n",
    "                        temp_premise = temp_premise.write(temp_premise.size(), tf.reduce_mean(chunks, axis = 0))\n",
    "                    else :\n",
    "                        temp_hypothesis = temp_hypothesis.write(temp_hypothesis.size(), tf.reduce_mean(chunks, axis = 0))\n",
    "                else :\n",
    "                    curr_len = temp_premise.size()\n",
    "                    for i in range(premise_len - curr_len) :\n",
    "                        temp_premise = temp_premise.write(temp_premise.size(), tf.zeros_like(x[batch, 0, :]))\n",
    "\n",
    "                    curr_len = temp_hypothesis.size()\n",
    "                    for i in range(hypo_len - curr_len) :\n",
    "                        temp_hypothesis = temp_hypothesis.write(temp_hypothesis.size(), tf.zeros_like(x[batch, 0, :]))\n",
    "\n",
    "            premise = premise.write(premise.size(), temp_premise.stack())\n",
    "            hypothesis = hypothesis.write(hypothesis.size(), temp_hypothesis.stack())\n",
    "        premise = premise.stack()\n",
    "        hypothesis = hypothesis.stack()\n",
    "        return premise, hypothesis\n",
    "        \n",
    "def syntax_struct_info_vec(w, gov_idx, tag) :\n",
    "    t = tf.TensorArray(tf.float32, size = 0, dynamic_size = True)\n",
    "    for batch in range(len(gov_idx)) :\n",
    "        temp = tf.TensorArray(tf.float32, size = 0, dynamic_size = True)\n",
    "        for idx in range(len(gov_idx[batch])) :\n",
    "            if gov_idx[batch][idx] >= 0 :\n",
    "                temp = temp.write(temp.size(), w[batch][idx] + w[batch][gov_idx[batch][idx] + idx] + tag[batch][idx])                    \n",
    "            else :\n",
    "                pad = tf.zeros_like(w[batch][idx])\n",
    "                temp = temp.write(temp.size(), pad)\n",
    "        temp = temp.stack()\n",
    "        t = t.write(t.size(), temp)\n",
    "    t = t.stack()\n",
    "    return t\n",
    "\n",
    "        \n",
    "class SyntaxStructureEmbedding(keras.layers.Layer) :\n",
    "    def __init__(self, input_dim, **kwargs) :\n",
    "        super(SyntaxStructureEmbedding, self).__init__()\n",
    "        self.embedder = keras.layers.Embedding(input_dim = input_dim, output_dim = 768)\n",
    "        \n",
    "    def call(self, x) :\n",
    "        return self.embedder(x)\n",
    "    \n",
    "\n",
    "class InfoConnectingLayer(keras.layers.Layer) :\n",
    "    def __init__(self, input_shape, rnn_size, **kwargs) :\n",
    "        super(InfoConnectingLayer, self).__init__()\n",
    "        \n",
    "        self.bias_shape = input_shape\n",
    "        self.u = keras.layers.Dense(self.bias_shape, activation = None, use_bias = False)\n",
    "        self.v_w = keras.layers.Dense(768, activation = None, use_bias = False)\n",
    "        self.v_t = keras.layers.Dense(768, activation = None, use_bias = False)\n",
    "        \n",
    "        self.bi_lstm = keras.layers.Bidirectional(keras.layers.LSTM(rnn_size))\n",
    "    \n",
    "    def build(self, input_shape) :\n",
    "        self.bias = self.add_weight(\"bias\",\n",
    "                                    shape = (self.bias_shape, 768))\n",
    "    \n",
    "    def call(self, w, t) :\n",
    "        t = self.u(t)\n",
    "        term_1 = tf.matmul(t, w)\n",
    "        term_2 = self.v_w(w)\n",
    "        term_3 = self.v_t(t)\n",
    "        \n",
    "        b = term_1 + term_2 + term_3 + self.bias\n",
    "        \n",
    "        return self.bi_lstm(b)\n",
    "    \n",
    "class ClassifierLayer(keras.layers.Layer) :\n",
    "    def __init__(self, rnn_size, n_class, **kwargs) :\n",
    "        super(ClassifierLayer, self).__init__()\n",
    "        self.u = keras.layers.Dense(rnn_size, activation = None, use_bias = False)\n",
    "        self.v_prem = keras.layers.Dense(rnn_size, activation = None, use_bias = False)\n",
    "        self.v_hypo = keras.layers.Dense(rnn_size, activation = None, use_bias = False)\n",
    "        self.outputs = keras.layers.Dense(n_class, activation = \"softmax\", use_bias = False)\n",
    "        \n",
    "    def call(self, h_prem, h_hypo) :\n",
    "        term_1 = self.u(tf.expand_dims(h_prem, 2))\n",
    "        term_1 = tf.matmul(term_1, tf.expand_dims(h_hypo, 2))\n",
    "        term_1 = keras.layers.Flatten()(term_1)\n",
    "        term_2 = self.v_prem(h_prem)\n",
    "        term_3 = self.v_hypo(h_hypo)\n",
    "        l = term_1 + term_2 + term_3\n",
    "        return self.outputs(l)\n",
    "    \n",
    "class SyntaxRoBERTa(keras.models.Model) :\n",
    "    def __init__(self, backbone, rnn_size, prem_max_len, hypo_max_len, **kargs) :\n",
    "        super(SyntaxRoBERTa, self).__init__()\n",
    "        \n",
    "        self.backbone = backbone\n",
    "        self.prem_tag_embedder = SyntaxStructureEmbedding(prem_max_len)\n",
    "        self.hypo_tag_embedder = SyntaxStructureEmbedding(hypo_max_len)\n",
    "        self.premise_info = InfoConnectingLayer(prem_max_len, rnn_size)\n",
    "        self.hypo_info = InfoConnectingLayer(hypo_max_len, rnn_size)\n",
    "        self.classifier = ClassifierLayer(rnn_size * 2, 3)\n",
    "        \n",
    "        self.prem_len = prem_max_len\n",
    "        self.hypo_len = hypo_max_len\n",
    "        \n",
    "    def call(self, x) :\n",
    "        backbone_input = {k : v for k, v in x.items() if k in [\"input_ids\", \"token_type_ids\", \"attention_mask\"]}\n",
    "        chunk_idx = x[\"chunk_index\"]\n",
    "        prem_gov_idx = x[\"prem_gov_idx\"]\n",
    "        hypo_gov_idx = x[\"hypo_gov_idx\"]\n",
    "        prem_tag = x[\"prem_tag\"]\n",
    "        hypo_tag = x[\"hypo_tag\"]\n",
    "        \n",
    "        prem_tag_emb = self.prem_tag_embedder(prem_tag)\n",
    "        hypo_tag_emb = self.hypo_tag_embedder(hypo_tag)\n",
    "        \n",
    "        seq_emb = self.backbone(backbone_input).last_hidden_state\n",
    "        p, h = chunk_mean(self.prem_len, self.hypo_len, seq_emb, chunk_idx, tf.where(x[\"input_ids\"] == 2)[0::2])\n",
    "\n",
    "        p_t = syntax_struct_info_vec(p, prem_gov_idx, prem_tag_emb)\n",
    "        h_t = syntax_struct_info_vec(h, hypo_gov_idx, hypo_tag_emb)\n",
    "        \n",
    "        p_i = self.premise_info(p, p_t)\n",
    "        h_i = self.hypo_info(h, h_t)\n",
    "        res = self.classifier(p_i, h_i)\n",
    "        \n",
    "        return res\n",
    "    \n",
    "batch_size = 2\n",
    "\n",
    "train_data = get_dataset(chunk_data, collator, batch_size, idx_max_len, True)\n",
    "train_data = train_data.map(lambda x, y : ({k : tf.cast(v, tf.int32) for k, v in x.items()}, tf.cast(y, tf.int32)),\n",
    "                            num_parallel_calls = tf.data.experimental.AUTOTUNE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "roberta = TFRobertaModel.from_pretrained(\"klue/roberta-base\", from_pt = True)\n",
    "roberta.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "model = SyntaxRoBERTa(roberta, 64, premise_max_len, hypo_max_len)\n",
    "loss_fn = keras.losses.sparse_categorical_crossentropy\n",
    "epochs = 5\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate = 1e-5)\n",
    "\n",
    "total_loss = []\n",
    "total_acc = []\n",
    "\n",
    "################## 훈련 루프 ###################\n",
    "\n",
    "for epoch in range(epochs) :\n",
    "        \n",
    "    cum_loss = deque(maxlen = 20)\n",
    "    cum_acc = deque(maxlen = 20)\n",
    "    \n",
    "    batch_loss = []\n",
    "    batch_acc = []\n",
    "    \n",
    "    with tqdm(train_data, unit = \"batch\") as tepoch :\n",
    "        for step, (x, y) in enumerate(tepoch) :\n",
    "            tepoch.set_description(f\"Epoch {epoch}\")\n",
    "            with tf.GradientTape() as t :\n",
    "                y_hat = model(x)\n",
    "                loss = loss_fn(y, y_hat)\n",
    "            dz_dx = t.gradient(loss, model.trainable_weights,\n",
    "                               unconnected_gradients = tf.UnconnectedGradients.ZERO)\n",
    "            optimizer.apply_gradients(zip(dz_dx, model.trainable_weights))\n",
    "            \n",
    "            curr_loss = float(tf.reduce_mean(loss))\n",
    "            curr_acc = float(keras.metrics.categorical_accuracy(y, tf.argmax(y_hat, axis = 1)))\n",
    "            \n",
    "            batch_loss.append(curr_loss)\n",
    "            batch_acc.append(curr_acc)\n",
    "            \n",
    "            cum_loss.append(curr_loss)\n",
    "            cum_acc.append(curr_acc)\n",
    "            \n",
    "            tepoch.set_postfix(loss = sum(cum_loss) / len(cum_loss),\n",
    "                               accuracy = sum(cum_acc) / len(cum_acc))      \n",
    "    total_loss.append(batch_loss)\n",
    "    total_acc.append(batch_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33852688-2434-4497-878f-154a406f79ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = Parser(API.KKMA)\n",
    "def make_chunk_pair(parser, sentence, chunk_token = \"[WORD]\") :\n",
    "    analyzed = parser(sentence)\n",
    "    \n",
    "    chunk = []\n",
    "    tag_info = []\n",
    "    n_words = []\n",
    "    gov_id = []\n",
    "    chunk_length = []\n",
    "\n",
    "    rel_gov_idx = []\n",
    "\n",
    "    for an in analyzed :\n",
    "        tmp_chunk = []\n",
    "        tmp_gov_id = []\n",
    "        tmp_idx = []\n",
    "        if len(an.words) > 1 :\n",
    "            words_a = an.words[:-1]\n",
    "            words_b = an.words[1:]\n",
    "\n",
    "            for f, s in zip(words_a, words_b) :\n",
    "                if (f.governorEdge.depType in [\"CMP\", \"MOD\", \"AJT\"]) and (s.governorEdge.depType in [\"CMP\", \"MOD\", \"AJT\"]) or \\\n",
    "                   (f.governorEdge.type == s.governorEdge.type)  and (f.governorEdge.type != 'X' or s.governorEdge.type != 'X') :\n",
    "                    tmp_chunk.append(f.surface)\n",
    "                else :\n",
    "                    tmp_chunk.append(f.surface)\n",
    "                    tmp_chunk.append(chunk_token)\n",
    "                    tmp_gov_id.append(f.governorEdge if f.governorEdge.src else 0)\n",
    "                    tag_info.append(f.governorEdge.type + '-' + f.governorEdge.depType if f.governorEdge.depType else f.governorEdge.type)\n",
    "\n",
    "            tmp_chunk.append(s.surface)\n",
    "            tmp_chunk.append(chunk_token)\n",
    "            tag_info.append(s.governorEdge.type + '-' + s.governorEdge.depType)\n",
    "            tmp_gov_id.append(s.governorEdge if s.governorEdge.src else 0)\n",
    "\n",
    "        else :\n",
    "            tmp_chunk.append(an.words[0].surface)\n",
    "            tmp_chunk.append(chunk_token)\n",
    "            tmp_gov_id.append(0)\n",
    "            tag_info.append(an.words[0].governorEdge.type + '-' + an.words[0].governorEdge.depType if an.words[0].governorEdge.depType else an.words[0].governorEdge.type)\n",
    "\n",
    "        tmp_words = np.cumsum(np.array(tmp_chunk) == chunk_token).tolist()\n",
    "        tmp_words = [tmp_words[i] for i in range(len(tmp_words)) if tmp_chunk[i] != chunk_token]\n",
    "\n",
    "        chunk_length.append(len(chunk[-1]) if chunk else 0)\n",
    "        n_words.append(tmp_words)\n",
    "        chunk.append(tmp_chunk)\n",
    "        to_get_rel_idx = tmp_gov_id.copy()\n",
    "\n",
    "        for idx in range(len(to_get_rel_idx)) :\n",
    "            if to_get_rel_idx[idx] != 0 :\n",
    "                edge = to_get_rel_idx[idx].src.id\n",
    "                tmp_gov_id[idx] = edge + tmp_words[edge] + sum(chunk_length)\n",
    "                tmp_idx.append(tmp_words[edge] - tmp_words[to_get_rel_idx[idx].dest.id])\n",
    "            else :\n",
    "                tmp_idx.append(0)\n",
    "        gov_id.append(tmp_gov_id)\n",
    "        rel_gov_idx.append(tmp_idx)\n",
    "\n",
    "    gov_id = [ids for sentence in gov_id for ids in sentence]\n",
    "    n_words = [n for sentence in n_words for n in sentence]\n",
    "    chunk = [c for sentence in chunk for c in sentence]\n",
    "    rel_gov_idx = [g for count in rel_gov_idx for g in count]\n",
    "    \n",
    "    result = {\"chunk\" : chunk,\n",
    "              \"chunk_tag\" : tag_info,\n",
    "              \"gov_idx\" : gov_id,\n",
    "              \"rel_gov_idx\" : rel_gov_idx}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d07f8-b412-429b-a540-0977d462464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parser = Parser(API.KKMA)\n",
    "from konlpy.tag import Okt\n",
    "sentence = original.premise[41]\n",
    "analyzed = parser(sentence)\n",
    "chunk_token = '[WORD]'\n",
    "    \n",
    "chunk = []\n",
    "tag_info = []\n",
    "n_words = []\n",
    "gov_id = []\n",
    "chunk_length = []\n",
    "\n",
    "rel_gov_idx = []\n",
    "\n",
    "for an in analyzed :\n",
    "    tmp_chunk = []\n",
    "    tmp_gov_id = []\n",
    "    tmp_idx = []\n",
    "\n",
    "\n",
    "\n",
    "    if len(an.words) > 1 :\n",
    "        words_a = an.words[:-1]\n",
    "        words_b = an.words[1:]\n",
    "\n",
    "        for f, s in zip(words_a, words_b) :\n",
    "            if (f.governorEdge.depType in [\"CMP\", \"MOD\", \"AJT\"]) and (s.governorEdge.depType in [\"CMP\", \"MOD\", \"AJT\"]) or \\\n",
    "               (f.governorEdge.type == s.governorEdge.type)  and (f.governorEdge.type != 'X' or s.governorEdge.type != 'X') :\n",
    "                tmp_chunk.append(f.surface)\n",
    "            else :\n",
    "                tmp_chunk.append(f.surface)\n",
    "                tmp_chunk.append(chunk_token)\n",
    "                tmp_gov_id.append(f.governorEdge if f.governorEdge.src else 0)\n",
    "                tag_info.append(f.governorEdge.type + '-' + f.governorEdge.depType if f.governorEdge.depType else f.governorEdge.type)\n",
    "\n",
    "        tmp_chunk.append(s.surface)\n",
    "        tmp_chunk.append(chunk_token)\n",
    "        tag_info.append(s.governorEdge.type + '-' + s.governorEdge.depType)\n",
    "        tmp_gov_id.append(s.governorEdge if s.governorEdge.src else 0)\n",
    "\n",
    "    else :\n",
    "        tmp_chunk.append(an.words[0].surface)\n",
    "        tmp_chunk.append(chunk_token)\n",
    "        tmp_gov_id.append(0)\n",
    "        tag_info.append(an.words[0].governorEdge.type + '-' + an.words[0].governorEdge.depType if an.words[0].governorEdge.depType else an.words[0].governorEdge.type)\n",
    "\n",
    "\n",
    "    tmp_words = np.cumsum(np.array(tmp_chunk) == chunk_token).tolist()\n",
    "    tmp_words = [tmp_words[i] for i in range(len(tmp_words)) if tmp_chunk[i] != chunk_token]\n",
    "\n",
    "    chunk_length.append(len(chunk[-1]) if chunk else 0)\n",
    "    n_words.append(tmp_words)\n",
    "    chunk.append(tmp_chunk)\n",
    "    to_get_rel_idx = tmp_gov_id.copy()\n",
    "\n",
    "\n",
    "    for idx in range(len(to_get_rel_idx)) :\n",
    "        if to_get_rel_idx[idx] != 0 :\n",
    "            edge = to_get_rel_idx[idx].src.id\n",
    "            tmp_gov_id[idx] = edge + tmp_words[edge] + sum(chunk_length)\n",
    "            tmp_idx.append(tmp_words[edge] - tmp_words[to_get_rel_idx[idx].dest.id])\n",
    "        else :\n",
    "            tmp_idx.append(0)\n",
    "    gov_id.append(tmp_gov_id)\n",
    "    rel_gov_idx.append(tmp_idx)\n",
    "\n",
    "gov_id = [ids for sentence in gov_id for ids in sentence]\n",
    "n_words = [n for sentence in n_words for n in sentence]\n",
    "chunk = [c for sentence in chunk for c in sentence]\n",
    "rel_gov_idx = [g for count in rel_gov_idx for g in count]\n",
    "\n",
    "\n",
    "    #####추가#####\n",
    "tagger = Okt()\n",
    "okt = tagger.morphs(sentence)\n",
    "copy = chunk.copy()\n",
    "\n",
    "ori_list = np.where(np.array(chunk) == \"[WORD]\")[0]\n",
    "res_list = ori_list - 1\n",
    "\n",
    "\n",
    "chunk_mark = []\n",
    "idx_mark = []\n",
    "cur_n_word = 0\n",
    "\n",
    "\n",
    "for j in range(len(res_list)) :\n",
    "    temp = []\n",
    "    cur_n_word -= 1\n",
    "    \n",
    "    if cur_n_word < 1 :\n",
    "        list_test.pop()\n",
    "        \n",
    "        cur_idx = j\n",
    "        cur_n_word = rel_gov_idx[j]\n",
    "            \n",
    "    for i in range(len(okt)) :\n",
    "        if okt[i] in chunk[res_list[j]] :\n",
    "            temp.append(1)\n",
    "        else : temp.append(0)\n",
    "        \n",
    "    if sum(temp) == 0 :\n",
    "        mark.append(ori_list[j])\n",
    "        idx_mark.append(j)\n",
    "        \n",
    "        rel_gov_idx[cur_idx] -= rel_gov_idx[j]\n",
    "\n",
    "rel_gov_idx = [rel_gov_idx[i] for i in range(len(rel_gov_idx)) if i not in idx_mark]\n",
    "tag_info = [tag_info[i] for i in range(len(tag_info)) if i not in idx_mark]\n",
    "chunk = [chunk[i] for i in range(len(chunk)) if i not in chunk_mark]\n",
    "        [idx_mark.append(l[0]) for l in list_test]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
